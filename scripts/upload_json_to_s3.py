#!/usr/bin/env python3
"""
Upload locally generated JSON files to S3 bucket.

This script reads JSON files from a local directory (generated by generate_s3_json_files.py)
and uploads them to the S3 bucket in the correct structure.
"""

import asyncio
import json
import sys
from pathlib import Path
from typing import Dict, List

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from app.clients.s3_json_storage import S3JSONStorage
from app.core.config import get_settings
from app.utils.logger import get_logger

settings = get_settings()
logger = get_logger(__name__)


async def upload_json_files(local_dir: Path, dry_run: bool = False) -> Dict[str, int]:
    """
    Upload JSON files from local directory to S3.
    
    Args:
        local_dir: Local directory containing generated JSON files
        dry_run: If True, only show what would be uploaded without actually uploading
    
    Returns:
        Dictionary with upload statistics
    """
    print("=" * 60)
    print("Upload JSON Files to S3")
    print("=" * 60)
    
    if dry_run:
        print("\nüîç DRY RUN MODE - No files will be uploaded\n")
    
    stats = {
        "uploaded": 0,
        "errors": 0,
        "skipped": 0,
    }
    
    if not local_dir.exists():
        print(f"\n‚ùå ERROR: Local directory not found: {local_dir}")
        return stats
    
    storage = S3JSONStorage()
    data_dir = local_dir / "data"
    
    if not data_dir.exists():
        print(f"\n‚ùå ERROR: 'data' directory not found in {local_dir}")
        print("   Expected structure: {local_dir}/data/pages/, data/endpoints/, etc.")
        return stats
    
    # Upload pages
    pages_dir = data_dir / "pages"
    if pages_dir.exists():
        print(f"\nüìÑ Uploading pages...")
        page_files = list(pages_dir.glob("*.json"))
        
        for page_file in sorted(page_files):
            try:
                # Read JSON file
                page_data = json.loads(page_file.read_text(encoding='utf-8'))
                
                # Determine S3 key
                if page_file.name == "index.json":
                    s3_key = f"{settings.S3_DATA_PREFIX}pages/index.json"
                else:
                    page_id = page_data.get("page_id", page_file.stem)
                    s3_key = f"{settings.S3_DATA_PREFIX}pages/{page_id}.json"
                
                if dry_run:
                    print(f"  [DRY RUN] Would upload: {page_file.name} -> {s3_key}")
                else:
                    await storage.write_json(s3_key, page_data)
                    print(f"  ‚úÖ Uploaded: {page_file.name} -> {s3_key}")
                
                stats["uploaded"] += 1
                
            except Exception as e:
                stats["errors"] += 1
                logger.error(f"Error uploading {page_file.name}: {e}", exc_info=True)
                print(f"  ‚ùå Error uploading {page_file.name}: {e}")
    
    # Upload endpoints
    endpoints_dir = data_dir / "endpoints"
    if endpoints_dir.exists():
        print(f"\nüì° Uploading endpoints...")
        endpoint_files = list(endpoints_dir.glob("*.json"))
        
        for endpoint_file in sorted(endpoint_files):
            try:
                endpoint_data = json.loads(endpoint_file.read_text(encoding='utf-8'))
                
                if endpoint_file.name == "index.json":
                    s3_key = f"{settings.S3_DATA_PREFIX}endpoints/index.json"
                else:
                    endpoint_id = endpoint_data.get("endpoint_id", endpoint_file.stem)
                    s3_key = f"{settings.S3_DATA_PREFIX}endpoints/{endpoint_id}.json"
                
                if dry_run:
                    print(f"  [DRY RUN] Would upload: {endpoint_file.name} -> {s3_key}")
                else:
                    await storage.write_json(s3_key, endpoint_data)
                    print(f"  ‚úÖ Uploaded: {endpoint_file.name} -> {s3_key}")
                
                stats["uploaded"] += 1
                
            except Exception as e:
                stats["errors"] += 1
                logger.error(f"Error uploading {endpoint_file.name}: {e}", exc_info=True)
                print(f"  ‚ùå Error uploading {endpoint_file.name}: {e}")
    
    # Upload relationships
    relationships_dir = data_dir / "relationships"
    if relationships_dir.exists():
        print(f"\nüîó Uploading relationships...")
        
        # Upload by-page relationships
        by_page_dir = relationships_dir / "by-page"
        if by_page_dir.exists():
            for rel_file in by_page_dir.glob("*.json"):
                try:
                    rel_data = json.loads(rel_file.read_text(encoding='utf-8'))
                    page_path = rel_data.get("page_path", "")
                    sanitized = page_path.strip("/").replace("/", "_").replace(":", "_") or "root"
                    s3_key = f"{settings.S3_DATA_PREFIX}relationships/by-page/{sanitized}.json"
                    
                    if dry_run:
                        print(f"  [DRY RUN] Would upload: {rel_file.name} -> {s3_key}")
                    else:
                        await storage.write_json(s3_key, rel_data)
                        print(f"  ‚úÖ Uploaded: {rel_file.name} -> {s3_key}")
                    
                    stats["uploaded"] += 1
                    
                except Exception as e:
                    stats["errors"] += 1
                    logger.error(f"Error uploading {rel_file.name}: {e}", exc_info=True)
                    print(f"  ‚ùå Error uploading {rel_file.name}: {e}")
        
        # Upload by-endpoint relationships
        by_endpoint_dir = relationships_dir / "by-endpoint"
        if by_endpoint_dir.exists():
            for rel_file in by_endpoint_dir.glob("*.json"):
                try:
                    rel_data = json.loads(rel_file.read_text(encoding='utf-8'))
                    endpoint_path = rel_data.get("endpoint_path", "")
                    method = rel_data.get("method", "GET")
                    sanitized_path = endpoint_path.strip("/").replace("/", "_").replace(":", "_")
                    sanitized_method = method.upper()
                    s3_key = f"{settings.S3_DATA_PREFIX}relationships/by-endpoint/{sanitized_path}_{sanitized_method}.json"
                    
                    if dry_run:
                        print(f"  [DRY RUN] Would upload: {rel_file.name} -> {s3_key}")
                    else:
                        await storage.write_json(s3_key, rel_data)
                        print(f"  ‚úÖ Uploaded: {rel_file.name} -> {s3_key}")
                    
                    stats["uploaded"] += 1
                    
                except Exception as e:
                    stats["errors"] += 1
                    logger.error(f"Error uploading {rel_file.name}: {e}", exc_info=True)
                    print(f"  ‚ùå Error uploading {rel_file.name}: {e}")
        
        # Upload relationships index
        rel_index_file = relationships_dir / "index.json"
        if rel_index_file.exists():
            try:
                index_data = json.loads(rel_index_file.read_text(encoding='utf-8'))
                s3_key = f"{settings.S3_DATA_PREFIX}relationships/index.json"
                
                if dry_run:
                    print(f"  [DRY RUN] Would upload: relationships/index.json -> {s3_key}")
                else:
                    await storage.write_json(s3_key, index_data)
                    print(f"  ‚úÖ Uploaded: relationships/index.json -> {s3_key}")
                
                stats["uploaded"] += 1
                
            except Exception as e:
                stats["errors"] += 1
                logger.error(f"Error uploading relationships/index.json: {e}", exc_info=True)
                print(f"  ‚ùå Error uploading relationships/index.json: {e}")
    
    # Summary
    print(f"\n{'='*60}")
    print("Upload Summary")
    print(f"{'='*60}")
    print(f"  ‚úÖ Uploaded:  {stats['uploaded']}")
    print(f"  ‚ùå Errors:   {stats['errors']}")
    print(f"  ‚è≠Ô∏è  Skipped:  {stats['skipped']}")
    print(f"\n  üì¶ S3 Bucket: {settings.S3_BUCKET_NAME}")
    print(f"  üìÇ S3 Prefix: {settings.S3_DATA_PREFIX}")
    print(f"{'='*60}\n")
    
    return stats


async def main():
    """Main entry point."""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Upload locally generated JSON files to S3 bucket"
    )
    
    parser.add_argument(
        "local_dir",
        type=str,
        nargs="?",
        default=str(Path(__file__).parent.parent / "generated_json"),
        help="Local directory containing generated JSON files (default: ./generated_json)"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Show what would be uploaded without actually uploading"
    )
    
    args = parser.parse_args()
    
    local_dir = Path(args.local_dir)
    
    try:
        stats = await upload_json_files(local_dir, dry_run=args.dry_run)
        
        if stats["errors"] > 0:
            sys.exit(1)
            
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Upload interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå Fatal error: {e}")
        logger.error(f"Fatal error in upload script: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
